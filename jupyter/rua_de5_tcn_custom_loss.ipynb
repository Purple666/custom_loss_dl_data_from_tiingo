{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use tiingo api to get stock prices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# settings.py\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# OR, the same with increased verbosity:\n",
    "load_dotenv(verbose=True)\n",
    "\n",
    "# OR, explicitly providing path to '.env'\n",
    "from pathlib import Path  # python3 only\n",
    "env_path = Path('D:\\python_code\\deep_learning_in_python\\stock\\numeric\\custom_loss_dl_data_from_tiingo') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "import os\n",
    "TIINGO_API_KEY = os.getenv(\"TIINGO_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用TCN加上custom loss function，與使用TCN, loss func使用MSE比較，為了減少Random seed的影響\\n，將每個參數組合跑10次後，平均，做出來的結果顯示加上custom loss func後，\\n報酬率平均比單純使用MSE作為損失函數好10%'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"使用TCN加上custom loss function，與使用TCN, loss func使用MSE比較，為了減少Random seed的影響\n",
    "，將每個參數組合跑10次後，平均，做出來的結果顯示加上custom loss func後，\n",
    "報酬率平均比單純使用MSE作為損失函數好10%\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker=\"SPY\"\n",
    "startDate=\"1990-01-01\"\n",
    "EndDate=\"2018-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "headers = {\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "requestResponse = requests.get(\n",
    "    \"https://api.tiingo.com/tiingo/daily/\"+ticker+\"/prices?startDate=\"+startDate+\"&endDate=\"+EndDate+\"&token=\"+TIINGO_API_KEY, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas\n",
    "\n",
    "jdata = json.loads(requestResponse.text)\n",
    "df = pandas.DataFrame(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190213\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\system32\\env002\\lib\\site-packages\\tensorflow\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(tf.__file__ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work: model  9 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot_ng\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpld3\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\python_code\\data\\us\\rua'\n",
    "\n",
    "fname = os.path.join(data_dir, 'rua_1988_2017_technical.csv')\n",
    "df = pd.read_csv(fname)\n",
    "df['Date'] = pd.to_datetime(df[\"Date\"])\n",
    "df_idx = df.set_index([\"Date\"], drop=True)\n",
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把順序調換\n",
    "df_idx = df_idx.sort_index(axis=0, ascending=False)\n",
    "df_idx = df_idx.iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idx.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_idx\n",
    "data.plot(y='Close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = data.index.values[-1] - data.index.values[0]\n",
    "days = diff.astype('timedelta64[D]')\n",
    "days = days / np.timedelta64(1, 'D')\n",
    "years = int(days/365)\n",
    "print(\"total data days:\",days)\n",
    "print(\"Total data: %d years\"%years)\n",
    "print(\"80 percent data = 1988 to %d\"%(1988 + int(0.8*years)))\n",
    "print(diff)\n",
    "delay = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#切割訓練與測試資料\n",
    "split_date = pd.Timestamp('01-01-2013')\n",
    "\n",
    "train = data.loc[:split_date]\n",
    "test = data.loc[split_date:]\n",
    "test_date = test.index\n",
    "test_date = pd.to_datetime(test_date)\n",
    "train_date = train.index\n",
    "train_date = pd.to_datetime(train_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料正規化\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "train_sc = sc.fit_transform(train)\n",
    "test_sc = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without sc\n",
    "#train_sc = train\n",
    "#test_sc = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df = pd.DataFrame(train_sc,index=train.index,columns=train.columns)\n",
    "test_sc_df = pd.DataFrame(test_sc,index=test.index,columns=test.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(-delay,-delay+1):\n",
    "    train_sc_df['Y_{}'.format(s)] = train_sc_df['Close'].shift(s)\n",
    "    test_sc_df['Y_{}'.format(s)] = test_sc_df['Close'].shift(s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = train_sc_df.dropna().drop('Y', axis=1)\n",
    "X_train = train_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_train = train_sc_df.dropna()['Y_-'+str(delay)]\n",
    "X_train = X_train.as_matrix()\n",
    "y_train = y_train.as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_sc_df.dropna().drop('Y_-'+str(delay), axis=1)\n",
    "y_test = test_sc_df.dropna().dropna()['Y_-'+str(delay)]\n",
    "X_test = X_test.as_matrix()\n",
    "y_test = y_test.as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size: (%d x %d)'%(X_train.shape[0], X_train.shape[1]))\n",
    "print('Test size: (%d x %d)'%(X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_imagepath ='images/loss/'\n",
    "predict_imagepath ='images/predict/'\n",
    "losspath = 'csv/loss/'\n",
    "if (not (os.path.exists(losspath))):\n",
    "        os.makedirs(losspath)\n",
    "model_dirpath = 'h5/'\n",
    "file_name='file_name'\n",
    "\n",
    "#for func initiate\n",
    "history_model='history_model'\n",
    "input_tensor='input_tensor'\n",
    "y_pred='y_pred'\n",
    "Target_DirPath='Target_DirPath'\n",
    "test_date_trim='test_date_trim'\n",
    "train_date_trim='train_date_trim'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Add,Reshape,Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping,CSVLogger\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras import Input,layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "K.clear_session()\n",
    "#from tcn import compiled_tcn,TCN\n",
    "\n",
    "def adj_r2_score(r2, n, k):\n",
    "    return 1-((1-r2)*((n-1)/(n-k-1)))\n",
    "\n",
    "X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "val_split_ratio = 0.1\n",
    "penalty=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_architecture(model, file_name):\n",
    "    file_path = 'images/model/{}.png'.format(file_name)\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(file_path))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "    plot_model(model, to_file=file_path, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmodel(model_dirpath=model_dirpath,file_name=file_name):\n",
    "    model = load_model(model_dirpath + file_name + '.h5')\n",
    "    return model\n",
    "def loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    loss = history_model.history['loss']\n",
    "    val_loss = history_model.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    if (not (os.path.exists(loss_imagepath))):\n",
    "                os.makedirs(loss_imagepath)\n",
    "    plt.savefig(loss_imagepath +  file_name +'_loss.png')\n",
    "    plt.show()    \n",
    "\n",
    "def CSV(losspath=losspath, file_name=file_name):\n",
    "    csv_logger = CSVLogger(losspath + file_name + '_log.csv')\n",
    "    return csv_logger\n",
    "def predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,\n",
    "                  predict_imagepath=predict_imagepath, file_name=file_name):\n",
    "    from sklearn.metrics import r2_score\n",
    "    y_pred = model.predict([X_tst_t])\n",
    "    #y_pred是三天前就知道，所以往前移三格\n",
    "    y_test_pic = y_test[:]\n",
    "    y_pred_pic = y_pred[delay:]\n",
    "    y_test_rsquare = y_test[:]\n",
    "    plt.plot(y_test_pic, label='True')\n",
    "    plt.plot(y_pred_pic, label='pred')\n",
    "    plt.xlabel('Observation')\n",
    "    plt.ylabel('Scaled_Value')\n",
    "    plt.legend()\n",
    "    print(\"y_pred.shape:\",y_pred.shape)\n",
    "    print(\"y_test_rsquare.shape:\",y_test_rsquare.shape)\n",
    "    r2_test = r2_score(y_test_rsquare, y_pred)\n",
    "    print('R-Squared: %f'%(r2_test))\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "          .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "    if (not (os.path.exists(predict_imagepath))):\n",
    "            os.makedirs(predict_imagepath)\n",
    "    plt.savefig(predict_imagepath +  file_name +'_loss.png')\n",
    "    plt.show()    \n",
    "    return y_pred\n",
    "def save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\\\n",
    "             file_name=file_name,test_date=test_date):\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_pred_data_like = np.zeros(shape=(len(y_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_data_like[:,0] = y_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_pred_data = sc.inverse_transform(y_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_test.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    test_date_trim = np.delete(test_date, np.s_[-delay:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(test_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'.csv', mode='w', header=True, index=False)\n",
    "    \n",
    "def save_train_csv(file_name=file_name):\n",
    "    y_train_pred = model.predict([X_tr_t])\n",
    "    # 把價格縮放解除\n",
    "    ## create empty table with label fields\n",
    "    y_train_pred_data_like = np.zeros(shape=(len(y_train_pred), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_train_pred_data_like[:,0] = y_train_pred[:,0]\n",
    "    ## inverse transform and then select the right field\n",
    "    y_train_pred_data = sc.inverse_transform(y_train_pred_data_like)[:,0]\n",
    "\n",
    "    #檢查測試資料的維度\n",
    "    yd_size = X_train.shape[0]\n",
    "\n",
    "    # 把價格轉換維度\n",
    "    yd = y_train_pred_data.reshape(yd_size,)\n",
    "\n",
    "    #把最後X天刪除(預測X天後)\n",
    "    train_date_trim = np.delete(train_date, np.s_[-delay:])\n",
    "\n",
    "    # 製作CSV\n",
    "    AnalysisResult = pd.DataFrame()\n",
    "    Date = pd.Series(train_date_trim)\n",
    "    Close = pd.Series(yd)\n",
    "    Date.name = 'Date'\n",
    "    Close.name = 'Close'\n",
    "\n",
    "    # 因為放在MC要開高低收，所以複製收盤填入\n",
    "    Open = Close.copy()\n",
    "    High = Close.copy()\n",
    "    Low = Close.copy()\n",
    "    Open.name = 'Open'\n",
    "    High.name = 'High'\n",
    "    Low.name = 'Low'\n",
    "\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Date], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Close], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Open], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,High], axis=1)\n",
    "    AnalysisResult = pd.concat([AnalysisResult,Low], axis=1)\n",
    "\n",
    "    # 輸出CSV檔案\n",
    "    import os\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master-train/'\n",
    "    if (not (os.path.exists(Target_DirPath))):\n",
    "            os.makedirs(Target_DirPath)\n",
    "    AnalysisResult.to_csv(Target_DirPath + file_name+'_train.csv', mode='w', header=True, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_of_skip_connections='selu'\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Activation, Lambda\n",
    "from tensorflow.keras.layers import Conv1D, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Convolution1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def channel_normalization(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\" Normalize a layer to the maximum activation\n",
    "\n",
    "    This keeps a layers values between zero and one.\n",
    "    It helps with relu's unbounded activation\n",
    "\n",
    "    Args:\n",
    "        x: The layer to normalize\n",
    "\n",
    "    Returns:\n",
    "        A maximal normalized layer\n",
    "    \"\"\"\n",
    "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
    "    out = x / max_values\n",
    "    return out\n",
    "\n",
    "\n",
    "def wave_net_activation(x):\n",
    "    # type: (Layer) -> Layer\n",
    "    \"\"\"This method defines the activation used for WaveNet\n",
    "\n",
    "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
    "\n",
    "    Args:\n",
    "        x: The layer we want to apply the activation to\n",
    "\n",
    "    Returns:\n",
    "        A new layer with the wavenet activation applied\n",
    "    \"\"\"\n",
    "    tanh_out = Activation('tanh')(x)\n",
    "    sigm_out = Activation('sigmoid')(x)\n",
    "    return tensorflow.keras.layers.multiply([tanh_out, sigm_out])\n",
    "\n",
    "\n",
    "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
    "    # type: (Layer, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
    "    \"\"\"Defines the residual block for the WaveNet TCN\n",
    "\n",
    "    Args:\n",
    "        x: The previous layer in the model\n",
    "        s: The stack index i.e. which stack in the overall TCN\n",
    "        i: The dilation power of 2 we are using for this residual block\n",
    "        activation: The name of the type of activation to use\n",
    "        nb_filters: The number of convolutional filters to use in this block\n",
    "        kernel_size: The size of the convolutional kernel\n",
    "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A tuple where the first element is the residual model layer, and the second\n",
    "        is the skip connection.\n",
    "    \"\"\"\n",
    "\n",
    "    original_x = x\n",
    "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
    "                  dilation_rate=i, padding=padding,\n",
    "                  name=name + '_d_%s_conv_%d_tanh_s%d' % (padding, i, s))(x)\n",
    "    if activation == 'norm_relu':\n",
    "        x = Activation('relu')(conv)\n",
    "        x = Lambda(channel_normalization)(x)\n",
    "    elif activation == 'wavenet':\n",
    "        x = wave_net_activation(conv)\n",
    "    else:\n",
    "        x = Activation(activation)(conv)\n",
    "\n",
    "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
    "\n",
    "    # 1x1 conv.\n",
    "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
    "    res_x = tensorflow.keras.layers.add([original_x, x])\n",
    "    return res_x, x\n",
    "\n",
    "\n",
    "def process_dilations(dilations):\n",
    "    def is_power_of_two(num):\n",
    "        return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
    "        return new_dilations\n",
    "\n",
    "\n",
    "class TCN:\n",
    "    \"\"\"Creates a TCN layer.\n",
    "\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=[1, 2, 4, 8, 16],\n",
    "                 activation='norm_relu',\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=True,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=True,\n",
    "                 name='tcn'):\n",
    "        self.name = name\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.activation = activation\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.padding = padding\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            print('An interface change occurred after the version 2.1.2.')\n",
    "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
    "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
    "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
    "            raise Exception()\n",
    "\n",
    "    def __call__(self, inputs, Activation_of_skip_connections='selu'):\n",
    "        x = inputs\n",
    "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
    "        skip_connections = []\n",
    "        for s in range(self.nb_stacks):\n",
    "            for i in self.dilations:\n",
    "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
    "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
    "                skip_connections.append(skip_out)\n",
    "        if self.use_skip_connections:\n",
    "            x = tensorflow.keras.layers.add(skip_connections)\n",
    "        x = Activation(Activation_of_skip_connections)(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            output_slice_index = -1\n",
    "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compiled_tcn(num_feat,  # type: int\n",
    "                 num_classes,  # type: int\n",
    "                 nb_filters,  # type: int\n",
    "                 kernel_size,  # type: int\n",
    "                 dilations,  # type: List[int]\n",
    "                 nb_stacks,  # type: int\n",
    "                 max_len,  # type: int\n",
    "                 activation='norm_relu',  # type: str\n",
    "                 padding='causal',  # type: str\n",
    "                 use_skip_connections=True,  # type: bool\n",
    "                 return_sequences=True,\n",
    "                 regression=False,  # type: bool\n",
    "                 dropout_rate=0.05,  # type: float\n",
    "                 name='tcn'  # type: str\n",
    "                 ):\n",
    "    # type: (...) -> tensorflow.keras.Model\n",
    "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
    "\n",
    "    Args:\n",
    "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
    "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
    "        nb_filters: The number of filters to use in the convolutional layers.\n",
    "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "        nb_stacks : The number of stacks of residual blocks to use.\n",
    "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
    "        activation: The activations to use.\n",
    "        padding: The padding to use in the convolutional layers.\n",
    "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
    "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        regression: Whether the output should be continuous or discrete.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "\n",
    "    Returns:\n",
    "        A compiled keras TCN.\n",
    "    \"\"\"\n",
    "\n",
    "    dilations = process_dilations(dilations)\n",
    "\n",
    "    input_layer = Input(shape=(max_len, num_feat))\n",
    "\n",
    "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
    "            padding, use_skip_connections, dropout_rate, return_sequences, name)(input_layer)\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "    if not regression:\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO To remove later.\n",
    "        def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
    "        print('Adam with norm clipping.')\n",
    "    else:\n",
    "        # regression\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('linear')(x)\n",
    "        output_layer = x\n",
    "        print(f'model.x = {input_layer.shape}')\n",
    "        print(f'model.y = {output_layer.shape}')\n",
    "        model = Model(input_layer, output_layer)\n",
    "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
    "        model.compile(adam, loss=tf_stock_loss_9)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_stock_loss_9(y_true, y_pred):\n",
    "    #indices = tf.constant([[0]])\n",
    "    #updates = tf.constant([False])\n",
    "    indices = tf.constant([[-1],[-2],[-3],[-4],[-5]])\n",
    "    updates = tf.cast(tf.constant([0,0,0,0,0]),tf.float32)\n",
    "\n",
    "    y_pred_tobemodified0 = tf.reshape(y_pred, [-1])\n",
    "    y_pred_tobemodified1 = tf.roll(y_pred_tobemodified0, shift=-5, axis=0)\n",
    "    y_pred_previous_cl = tf.tensor_scatter_update(y_pred_tobemodified0, indices, updates)\n",
    "    y_pred_cl = tf.tensor_scatter_update(y_pred_tobemodified1, indices, updates)\n",
    "    y_pred_previous = tf.reshape(y_pred_previous_cl,tf.shape(y_pred))\n",
    "    y_pred_dummy = tf.reshape(y_pred_cl,tf.shape(y_pred))\n",
    "    \n",
    "    y_pred_rising = K.less(y_pred_previous, y_pred_dummy)\n",
    "    y_pred_falling = K.greater(y_pred_previous, y_pred_dummy)    \n",
    "\n",
    "\n",
    "    y_true_tobemodified0 = tf.reshape(y_true, [-1])\n",
    "    y_true_tobemodified1 = tf.roll(y_true_tobemodified0, shift=-5, axis=0)\n",
    "    y_true_previous_cl = tf.tensor_scatter_update(y_pred_tobemodified0, indices, updates)\n",
    "    y_true_cl = tf.tensor_scatter_update(y_true_tobemodified1, indices, updates)\n",
    "    y_true_previous = tf.reshape(y_true_previous_cl,tf.shape(y_true))\n",
    "    y_true_dummy = tf.reshape(y_true_cl,tf.shape(y_true))\n",
    "    \n",
    "    y_true_rising = K.less(y_true_previous, y_true_dummy)\n",
    "    y_true_falling = K.greater(y_true_previous, y_true_dummy)  \n",
    "\n",
    "    loss = tf.keras.backend.cast(tf.logical_and(y_true_falling,y_pred_rising),dtype='float32')*penalty* mean_squared_error(y_true, y_pred)+\\\n",
    "             tf.keras.backend.cast(tf.logical_and(y_true_rising,y_pred_falling),dtype='float32')*penalty*mean_squared_error(y_true, y_pred)+\\\n",
    "             mean_squared_error(y_true, y_pred)       \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_r2_score(file_name):\n",
    "    Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "    file_end_with = '.csv'\n",
    "    #above need to be adjust\n",
    "    df_for_eval= pd.read_csv(Target_DirPath + file_name + file_end_with)\n",
    "    df_for_eval['Date'] = pd.to_datetime(df_for_eval[\"Date\"])\n",
    "    df_for_eval = df_for_eval.set_index([\"Date\"], drop=True)\n",
    "\n",
    "    y_pred_for_eval = df_for_eval[\"Close\"]\n",
    "    print(y_pred_for_eval.shape)\n",
    "    ## create empty table with label fields\n",
    "    y_pred_for_eval_data_like = np.zeros(shape=(len(y_pred_for_eval), X_train.shape[1]))\n",
    "    ## put the predicted values in the right field\n",
    "    y_pred_for_eval_data_like[:,0] = y_pred_for_eval[:]\n",
    "    ## transform and then select the right field\n",
    "    y_pred_for_eval_data = sc.transform(y_pred_for_eval_data_like)[:,0]\n",
    "\n",
    "    r2_test = r2_score(y_test, y_pred_for_eval_data)\n",
    "    print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "    y_test_diff = np.diff(y_test) #y_test[i]與y_test[i-1]差異\n",
    "    y_pred_for_eval_data_diff = np.diff(y_pred_for_eval_data)#y_pred[i]與y_pred[i-1]差異\n",
    "    rsquare_product = y_test_diff*y_pred_for_eval_data_diff #兩者相乘\n",
    "    def return_same_sign_bool(d):\n",
    "        d = np.array(d)\n",
    "        return np.where(d > 0, 1, 0)\n",
    "    rsquare_product_bool = return_same_sign_bool(rsquare_product) #如果兩者相乘為正數回傳1，非正數回傳0\n",
    "    print(\"The Custom  sign score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))) \n",
    "               #計算y_test及y_pred變動同向機率\n",
    "    print(\"The Custom  R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "              .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])/2 \n",
    "                      + ((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))/2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[32]\n",
    "penalties=[0.01, 0.1]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[32]\n",
    "penalties=[1, 10]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[32]\n",
    "penalties=[100, 'x']\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[32]\n",
    "penalties=[100]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(6 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[32]\n",
    "penalties=['x']\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[128]\n",
    "penalties=[0.01, 0.1]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[128]\n",
    "penalties=[1, 10]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[128]\n",
    "penalties=[100, 'x']\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neurons=[512] are undone\n",
    "neurons=[512]\n",
    "penalties=[0.01, 0.1]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[512]\n",
    "penalties=[1, 10]\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons=[512]\n",
    "penalties=[100, 'x']\n",
    "losses = tf_stock_loss_9\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        for i in range(0 ,10):\n",
    "            if penalty_i == 'x':\n",
    "                losses = 'mse'\n",
    "            else:\n",
    "                losses = tf_stock_loss_9\n",
    "                penalty = penalty_i\n",
    "            file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)+\"_\"+str(i)\n",
    "\n",
    "            X_tr_t = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "            X_tst_t = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "            input_tensor= Input(shape=(1,X_train.shape[1],))\n",
    "            output = TCN(nb_filters=neurons_j, kernel_size=2, nb_stacks=1, dilations=[1, 2, 4, 8, 16], \\\n",
    "                    activation='selu', padding='causal', use_skip_connections=True,\\\n",
    "                    dropout_rate=0.3, return_sequences=False, name='tcn')(input_tensor)  # The TCN layers are here.\n",
    "\n",
    "            output_tensor = Dense(1)(output)\n",
    "            model = Model([input_tensor], output_tensor)\n",
    "            model.compile(optimizer=Adam(lr=0.0001, clipnorm=1, clipvalue=0.5), loss=losses)\n",
    "            model.summary()\n",
    "            early_stop = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
    "            history_model = model.fit(x=[X_tr_t], y=y_train, epochs=300, \n",
    "                                                batch_size=32, verbose=1,\n",
    "                                                validation_split= val_split_ratio,\n",
    "                                                shuffle=False)\n",
    "            plot_model_architecture(model=model, file_name=file_name)\n",
    "            y_pred = predict_image(history_model=history_model,X_tst_t=X_tst_t,input_tensor=input_tensor,          \n",
    "                              predict_imagepath=predict_imagepath, file_name=file_name)\n",
    "            loss_image(history_model=history_model, loss_imagepath=loss_imagepath, file_name=file_name)\n",
    "            save_csv(y_pred=y_pred,X_train=X_train,X_test=X_test,Target_DirPath=Target_DirPath,\n",
    "                         file_name=file_name,test_date=test_date)\n",
    "            save_train_csv(file_name=file_name)\n",
    "            print(file_name+\":\")\n",
    "            custom_r2_score(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sign score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "neurons=[32,128]\n",
    "penalties=[0.01, 0.1, 1, 10, 100, 'x']\n",
    "\n",
    "for neurons_j in neurons:\n",
    "    for penalty_i in penalties:\n",
    "        if penalty_i == 'x':\n",
    "            losses = 'mse'\n",
    "        else:\n",
    "            penalty = penalty_i\n",
    "        file_name='RUA_de5_tcn_clf9_n'+str(neurons_j)+'_pe'+str(penalty_i)\n",
    "        print(\"=============================================\")\n",
    "        print(file_name)\n",
    "        Target_DirPath = 'Deep-Learning-in-Python-master/'\n",
    "        file_end_with = '.csv'\n",
    "        #above need to be adjust\n",
    "        df_for_eval= pd.read_csv(Target_DirPath + file_name + file_end_with)\n",
    "        df_for_eval['Date'] = pd.to_datetime(df_for_eval[\"Date\"])\n",
    "        df_for_eval = df_for_eval.set_index([\"Date\"], drop=True)\n",
    "\n",
    "        y_pred_for_eval = df_for_eval[\"Close\"]\n",
    "        ## create empty table with label fields\n",
    "        y_pred_for_eval_data_like = np.zeros(shape=(len(y_pred_for_eval), X_train.shape[1]))\n",
    "        ## put the predicted values in the right field\n",
    "        y_pred_for_eval_data_like[:,0] = y_pred_for_eval[:]\n",
    "        ## transform and then select the right field\n",
    "        y_pred_for_eval_data = sc.transform(y_pred_for_eval_data_like)[:,0]\n",
    "\n",
    "\n",
    "        r2_test = r2_score(y_test, y_pred_for_eval_data)\n",
    "        print(\"The Adjusted R2 score on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format(adj_r2_score(r2_test, X_test.shape[0], X_test.shape[1])))\n",
    "\n",
    "        y_test_diff = np.diff(y_test) #y_test[i]與y_test[i-1]差異\n",
    "        y_pred_for_eval_data_diff = np.diff(y_pred_for_eval_data)#y_pred[i]與y_pred[i-1]差異\n",
    "        rsquare_product = y_test_diff*y_pred_for_eval_data_diff #兩者相乘\n",
    "        def return_same_sign_bool(d):\n",
    "            d = np.array(d)\n",
    "            return np.where(d > 0, 1, 0)\n",
    "        rsquare_product_bool = return_same_sign_bool(rsquare_product) #如果兩者相乘為正數回傳1，非正數回傳0\n",
    "        print(\"The Custom  sign score on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format((int(sum(rsquare_product_bool)) / len(rsquare_product_bool)))) \n",
    "                   #計算y_test及y_pred變動同向機率\n",
    "\n",
    "        #Custom  sign score_2\n",
    "        y_test_after = np.roll(y_test,-5) #y_test[i]與y_test[i-1]差異\n",
    "        y_pred_for_eval_data_after = np.roll(y_pred_for_eval_data,-5)#y_pred[i]與y_pred[i-1]差異\n",
    "        y_test__cl = np.subtract(y_test_after,y_test)\n",
    "        y_pred_for_eval_data_cl = np.subtract(y_pred_for_eval_data_after,y_pred_for_eval_data)\n",
    "\n",
    "        sign_raw = np.multiply(y_test__cl, y_pred_for_eval_data_cl)\n",
    "\n",
    "        sign_bool_func = np.vectorize(lambda elements : 1 if elements <= 0  else 0)\n",
    "        sign_bool = sign_bool_func(sign_raw)\n",
    "        sign_bool[np.r_[-5:-1]] = 0\n",
    "\n",
    "        print(\"The Custom  sign score_2 on the Test set is:\\t{:0.3f}\"\\\n",
    "                  .format((int(sum(sign_bool)) / (len(sign_bool)-5)))) \n",
    "                   #計算y_test及y_pred變動同向機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.constant([[[0,0],[0,1]]])\n",
    "updates = tf.constant([[[5, 5, 5, 5],[5, 5, 5, 5]]])\n",
    "shape = tf.constant([4, 4, 4])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    "print(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = tf.constant([[[0,0],[0,1]]])\n",
    "updates = tf.constant([[5, 5]])\n",
    "shape = tf.constant([3, 5])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    "print(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de = list()\n",
    "de = [0,2,6,7,8,9]\n",
    "de[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env002",
   "language": "python",
   "name": "env002"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
